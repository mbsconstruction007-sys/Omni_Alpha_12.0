"""
Hybrid AI Brain with automatic fallback for Windows compatibility
"""

import sys
import platform
import logging
from typing import Dict, Any, Optional
import numpy as np
import pandas as pd
import asyncio
from datetime import datetime

logger = logging.getLogger(__name__)

class HybridAIBrain:
    """
    Intelligent AI Brain that works on any platform
    """
    
    def __init__(self):
        self.backend = self._initialize_backend()
        self.mode = "hybrid"
        self.thoughts = []
        self.consciousness_level = 0.0
        
    def _initialize_backend(self):
        """Initialize the best available backend"""
        system = platform.system()
        logger.info(f"ðŸ–¥ï¸ Detected platform: {system}")
        
        # Try PyTorch first
        try:
            import torch
            # Test if PyTorch actually works
            test_tensor = torch.randn(1, 1)
            logger.info("âœ… PyTorch backend initialized successfully")
            return PyTorchBackend()
        except (ImportError, OSError, Exception) as e:
            logger.warning(f"PyTorch not available or failed: {e}")
        
        # Try TensorFlow as alternative
        try:
            import tensorflow as tf
            # Test if TensorFlow actually works
            test_tensor = tf.constant([1, 2, 3])
            logger.info("âœ… TensorFlow backend initialized")
            return TensorFlowBackend()
        except (ImportError, OSError, Exception) as e:
            logger.warning(f"TensorFlow not available or failed: {e}")
        
        # Fallback to NumPy-based implementation
        logger.info("âœ… NumPy backend initialized (fallback mode)")
        return NumpyBackend()
    
    async def think(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process thoughts using best available backend"""
        thought = {
            "timestamp": datetime.now().isoformat(),
            "input": input_data,
            "backend": self.backend.__class__.__name__
        }
        
        try:
            result = await self.backend.process(input_data)
            thought["result"] = result
            thought["success"] = True
            self.thoughts.append(thought)
            self.consciousness_level = min(1.0, self.consciousness_level + 0.01)
            return result
        except Exception as e:
            thought["error"] = str(e)
            thought["success"] = False
            self.thoughts.append(thought)
            logger.error(f"Thinking failed: {e}")
            return {"error": str(e), "backend": "error"}
    
    async def evolve(self) -> Dict[str, Any]:
        """Evolve the brain"""
        evolution_result = {
            "timestamp": datetime.now().isoformat(),
            "consciousness_level": self.consciousness_level,
            "thoughts_count": len(self.thoughts),
            "backend": self.backend.__class__.__name__
        }
        
        # Simulate evolution
        self.consciousness_level = min(1.0, self.consciousness_level + 0.1)
        
        return evolution_result
    
    async def dream(self) -> Dict[str, Any]:
        """Dream state for exploration"""
        dream_result = {
            "timestamp": datetime.now().isoformat(),
            "insights": [
                "Market patterns are fractal",
                "Volatility creates opportunity",
                "Risk and reward are inseparable"
            ],
            "consciousness_level": self.consciousness_level
        }
        
        return dream_result

class PyTorchBackend:
    """Full PyTorch implementation"""
    
    def __init__(self):
        try:
            import torch
            import torch.nn as nn
            
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model = self._build_model()
            logger.info(f"PyTorch device: {self.device}")
        except Exception as e:
            logger.error(f"PyTorch backend initialization failed: {e}")
            raise
    
    def _build_model(self):
        import torch.nn as nn
        
        class TradingBrain(nn.Module):
            def __init__(self):
                super().__init__()
                self.lstm = nn.LSTM(100, 256, 2, batch_first=True)
                self.attention = nn.MultiheadAttention(256, 8)
                self.fc = nn.Sequential(
                    nn.Linear(256, 512),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 3)  # Buy, Hold, Sell
                )
            
            def forward(self, x):
                lstm_out, _ = self.lstm(x)
                attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
                return self.fc(attn_out[:, -1, :])
        
        return TradingBrain().to(self.device)
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process using PyTorch model"""
        import torch
        
        # Convert input to tensor
        x = torch.randn(1, 10, 100).to(self.device)
        
        # Forward pass
        with torch.no_grad():
            output = self.model(x)
            probabilities = torch.softmax(output, dim=1)
        
        return {
            "backend": "pytorch",
            "decision": ["SELL", "HOLD", "BUY"][probabilities.argmax().item()],
            "confidence": probabilities.max().item(),
            "device": str(self.device),
            "probabilities": {
                "SELL": probabilities[0][0].item(),
                "HOLD": probabilities[0][1].item(),
                "BUY": probabilities[0][2].item()
            }
        }

class TensorFlowBackend:
    """TensorFlow alternative implementation"""
    
    def __init__(self):
        try:
            import tensorflow as tf
            
            self.model = self._build_model()
            logger.info("TensorFlow backend ready")
        except Exception as e:
            logger.error(f"TensorFlow backend initialization failed: {e}")
            raise
    
    def _build_model(self):
        import tensorflow as tf
        
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(256, return_sequences=True),
            tf.keras.layers.LSTM(128),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(3, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process using TensorFlow model"""
        import tensorflow as tf
        
        # Create dummy input
        x = tf.random.normal((1, 10, 100))
        
        # Predict
        output = self.model.predict(x, verbose=0)
        decision_idx = output.argmax()
        
        return {
            "backend": "tensorflow",
            "decision": ["SELL", "HOLD", "BUY"][decision_idx],
            "confidence": float(output.max()),
            "probabilities": {
                "SELL": float(output[0][0]),
                "HOLD": float(output[0][1]),
                "BUY": float(output[0][2])
            }
        }

class NumpyBackend:
    """Pure NumPy implementation for maximum compatibility"""
    
    def __init__(self):
        self.weights = self._initialize_weights()
        logger.info("NumPy backend ready (fallback mode)")
    
    def _initialize_weights(self):
        """Initialize random weights"""
        np.random.seed(42)
        return {
            "W1": np.random.randn(100, 256) * 0.01,
            "b1": np.zeros((1, 256)),
            "W2": np.random.randn(256, 128) * 0.01,
            "b2": np.zeros((1, 128)),
            "W3": np.random.randn(128, 3) * 0.01,
            "b3": np.zeros((1, 3))
        }
    
    def relu(self, x):
        """ReLU activation"""
        return np.maximum(0, x)
    
    def softmax(self, x):
        """Softmax activation"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / exp_x.sum()
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process using NumPy neural network"""
        # Create input based on market data
        symbol = input_data.get("symbol", "UNKNOWN")
        price = input_data.get("price", 100)
        volume = input_data.get("volume", 1000000)
        
        # Create feature vector
        features = np.array([
            price / 1000,  # Normalized price
            volume / 10000000,  # Normalized volume
            np.random.randn(98)  # Random market features
        ]).reshape(1, 100)
        
        # Forward pass
        z1 = np.dot(features, self.weights["W1"]) + self.weights["b1"]
        a1 = self.relu(z1)
        
        z2 = np.dot(a1, self.weights["W2"]) + self.weights["b2"]
        a2 = self.relu(z2)
        
        z3 = np.dot(a2, self.weights["W3"]) + self.weights["b3"]
        output = self.softmax(z3)
        
        decision_idx = output.argmax()
        decisions = ["SELL", "HOLD", "BUY"]
        
        return {
            "backend": "numpy",
            "decision": decisions[decision_idx],
            "confidence": float(output.max()),
            "fallback_mode": True,
            "symbol": symbol,
            "probabilities": {
                "SELL": float(output[0][0]),
                "HOLD": float(output[0][1]),
                "BUY": float(output[0][2])
            },
            "reasoning": f"NumPy neural network analysis for {symbol}"
        }

# ============================================
# WINDOWS-COMPATIBLE EXECUTION ENGINE
# ============================================

class UniversalExecutionEngine:
    """
    Execution engine that works on any platform
    """
    
    def __init__(self):
        self.brain = HybridAIBrain()
        self.execution_history = []
        self.performance_metrics = {
            "total_orders": 0,
            "successful_orders": 0,
            "total_pnl": 0.0,
            "avg_latency_ms": 0.0
        }
        
    async def execute_with_ai(self, order_params: Dict[str, Any]) -> Dict[str, Any]:
        """Execute order with AI decision support"""
        start_time = datetime.now()
        
        # Get AI decision
        ai_decision = await self.brain.think(order_params)
        
        # Simulate execution based on AI recommendation
        if ai_decision.get("confidence", 0) > 0.6:
            execution_result = {
                "status": "executed",
                "ai_decision": ai_decision.get("decision", "HOLD"),
                "confidence": ai_decision.get("confidence", 0.0),
                "backend_used": ai_decision.get("backend", "unknown"),
                "order_id": f"ORD_{np.random.randint(100000, 999999)}",
                "fill_price": order_params.get("price", 100) * (1 + np.random.randn() * 0.001),
                "timestamp": datetime.now().isoformat(),
                "latency_ms": (datetime.now() - start_time).total_seconds() * 1000,
                "probabilities": ai_decision.get("probabilities", {}),
                "reasoning": ai_decision.get("reasoning", "AI analysis")
            }
            
            # Update metrics
            self.performance_metrics["total_orders"] += 1
            self.performance_metrics["successful_orders"] += 1
            self.performance_metrics["total_pnl"] += np.random.randn() * 100
            
        else:
            execution_result = {
                "status": "rejected",
                "reason": "Low AI confidence",
                "confidence": ai_decision.get("confidence", 0.0),
                "backend_used": ai_decision.get("backend", "unknown"),
                "timestamp": datetime.now().isoformat(),
                "latency_ms": (datetime.now() - start_time).total_seconds() * 1000
            }
            
            self.performance_metrics["total_orders"] += 1
        
        self.execution_history.append(execution_result)
        
        # Update average latency
        latencies = [ex.get("latency_ms", 0) for ex in self.execution_history[-100:]]
        self.performance_metrics["avg_latency_ms"] = np.mean(latencies)
        
        return execution_result
    
    async def get_performance_metrics(self) -> Dict[str, Any]:
        """Get execution performance metrics"""
        return {
            **self.performance_metrics,
            "success_rate": (
                self.performance_metrics["successful_orders"] / 
                max(1, self.performance_metrics["total_orders"])
            ),
            "backend": self.brain.backend.__class__.__name__,
            "consciousness_level": self.brain.consciousness_level,
            "thoughts_count": len(self.brain.thoughts)
        }

# ============================================
# COMPATIBILITY TEST SUITE
# ============================================

async def test_hybrid_system():
    """Test the hybrid AI system"""
    print("\n" + "="*60)
    print("ðŸ§ª TESTING HYBRID AI BRAIN SYSTEM")
    print("="*60)
    
    # Initialize system
    engine = UniversalExecutionEngine()
    
    # Test order execution
    test_orders = [
        {"symbol": "AAPL", "quantity": 100, "price": 150, "side": "BUY", "volume": 2000000},
        {"symbol": "GOOGL", "quantity": 50, "price": 2800, "side": "SELL", "volume": 500000},
        {"symbol": "TSLA", "quantity": 75, "price": 250, "side": "BUY", "volume": 3000000}
    ]
    
    for order in test_orders:
        result = await engine.execute_with_ai(order)
        print(f"\nðŸ“Š Order: {order['symbol']} {order['side']} {order['quantity']}")
        print(f"   Backend: {result.get('backend_used', 'Unknown')}")
        print(f"   Status: {result['status']}")
        if result['status'] == 'executed':
            print(f"   AI Decision: {result['ai_decision']}")
            print(f"   Confidence: {result['confidence']:.2%}")
            print(f"   Fill Price: ${result['fill_price']:.2f}")
            print(f"   Latency: {result['latency_ms']:.2f}ms")
        else:
            print(f"   Reason: {result.get('reason', 'Unknown')}")
    
    # Get performance metrics
    metrics = await engine.get_performance_metrics()
    print(f"\nðŸ“ˆ Performance Metrics:")
    print(f"   Total Orders: {metrics['total_orders']}")
    print(f"   Success Rate: {metrics['success_rate']:.2%}")
    print(f"   Avg Latency: {metrics['avg_latency_ms']:.2f}ms")
    print(f"   Backend: {metrics['backend']}")
    print(f"   Consciousness Level: {metrics['consciousness_level']:.2f}")
    
    print("\n" + "="*60)
    print("âœ… HYBRID SYSTEM TEST COMPLETE")
    print("="*60)
    
    return True

if __name__ == "__main__":
    asyncio.run(test_hybrid_system())
